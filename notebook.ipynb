{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle Detection\n",
    "\n",
    "Vehicle detection is an important process in any\n",
    "autonomous driving pipeline. Once the vehicles are identified they can be used to\n",
    "plan how an autonomous vehicle will navigate an enviornment. This will be a tutorial on how to implement a vehicle identifier algorithm using sklearn and OpenCV. The steps of this algortihm are:\n",
    "\n",
    "* Perform feature extraction on a labeled training set of images and train a classifier.\n",
    "* Implement a sliding-window technique and use our trained classifier to search for vehicles in images.\n",
    "* Run our pipeline on a video stream and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "The first and arguably most important part of a machine learning pipeline is the extraction of good features that proved a rich representation of our objects of interest. We want our classifier to be able to detect cars so we shall use the following features:\n",
    "\n",
    "* A histogram of oriented gradients (HOG) for each channel.\n",
    "* A histgram of each color channel.\n",
    "* A spatial bin of each color channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.feature import hog\n",
    "\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block,feature_vec=True):\n",
    "\n",
    "    features = hog(img, orientations=orient,\n",
    "                   pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                   cells_per_block=(cell_per_block, cell_per_block),\n",
    "                   transform_sqrt=True,\n",
    "                   visualise=False, feature_vector=feature_vec)\n",
    "    return features\n",
    "\n",
    "def color_hist(img, nbins=32):\n",
    "    channel1_hist = np.histogram(img[:,:,0], bins=nbins)\n",
    "    channel2_hist = np.histogram(img[:,:,1], bins=nbins)\n",
    "    channel3_hist = np.histogram(img[:,:,2], bins=nbins)\n",
    "    hist_features = np.concatenate([channel1_hist[0], channel2_hist[0], channel3_hist[1]])\n",
    "    return hist_features\n",
    "\n",
    "def bin_spatial(img, size=(32,32)):\n",
    "    channel1 = cv2.resize(img[:,:,0], size).ravel()\n",
    "    channel2 = cv2.resize(img[:,:,1], size).ravel()\n",
    "    channel3 = cv2.resize(img[:,:,2], size).ravel()\n",
    "    return np.hstack((channel1, channel2, channel3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our implemetation we have chosen the following parameters for feature extraction:\n",
    "* Convert from RGB color space to YCrCb color space\n",
    "* HOG: 8 oreintation bins, (8,8) pixels pers cell, and 2 cells per block\n",
    "* Spatial binning dimenstions: (32,32)\n",
    "* Color Histogram: 32 histogram bins\n",
    "\n",
    "These parameters were chosen after tuning on multiple test images and seeing which produced the best ratio of true postitives to false positives. Below is an illustration of how HOG differentiates between car and non-car images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<figure>\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/HOG.png\" width=\"900\"\n",
    "alt=\"Car Image\" />\n",
    "</p>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to choosing a classifier there are a variety of options. Linear Support Vector Machines (SVM) have been shown to work pretty well with HOG features so we'll try that one. As always we split our data into a training and test sets and train on the training set. One feature of SVMs is that they also have a \"decision_function\" method which can be interpreted as the confidence of our classifier this will be important when we use our classifier to classify new images.\n",
    "\n",
    "Also before we train our classifier it is important to normalize our features. We do this so that one feature doesnt dominate the others because it just happens to have a broader scale.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.vstack((car_features, notcar_features)).astype(np.float64)\n",
    "# Fit a per-column scaler\n",
    "X_scaler = StandardScaler().fit(X)\n",
    "# Apply the scaler to X\n",
    "scaled_X = X_scaler.transform(X)\n",
    "\n",
    "# Define the labels vector\n",
    "y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))\n",
    "\n",
    "rand_state = np.random.randint(0, 100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    scaled_X, y, test_size=0.2, random_state=rand_state)\n",
    "\n",
    "svc = LinearSVC()\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Windows\n",
    "Now that our classifier is trained we want to classify images in our video stream. To do that we will subsample each image in a region of interest to us into different window sizes and feed them into our classfiier. We'll use 64x64 and 96x96 windows with a bit of overlap as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/search_windows.png\" width=\"900\"\n",
    "alt=\"Car Image\" />\n",
    "</p>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we will find when we test on images is that we get some false positives. We can do a few things to alleviate this problem. One thins is using the \"decision_function\" from before and thresholding on the returned value. Another thing to notice is that a vehicles will be matched in multiple windows in the image. We can define a heat map of these windows and use a threshold to get rid of isolated windows which usually correspond to false positives. Once we have a thresholded heat map we can use a function in scipy called \"label\" to find the connected regions in our heat map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage.measurements import label\n",
    "\n",
    "def draw_labeled_bboxes(img, labels):\n",
    "\n",
    "    for car_number in range(1, labels[1]+1):\n",
    "\n",
    "        nonzero = (labels[0] == car_number).nonzero()\n",
    "\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "\n",
    "        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "\n",
    "        cv2.rectangle(img, bbox[0], bbox[1], (0,0,255),6)\n",
    "\n",
    "    return img\n",
    "\n",
    "def process_image(img_src, vis_heat=False):\n",
    "\n",
    "    draw_img = np.copy(img_src)\n",
    "    scales = [1,1.5]\n",
    "    img_src = img_src.astype(np.float32)/255\n",
    "    heat_map = np.zeros_like(img_src[:,:,0])\n",
    "\n",
    "    img_src = cv2.cvtColor(img_src, cv2.COLOR_RGB2YCrCb)\n",
    "    img_cropped = img_src[y_start_stop[0]:y_start_stop[1],:,:]\n",
    "\n",
    "    for scale in scales:\n",
    "        if scale != 1:\n",
    "            imshape = img_cropped.shape\n",
    "            img_cropped = cv2.resize(img_cropped, (np.int(imshape[1]/scale), np.int(imshape[0]/scale)))\n",
    "\n",
    "        ch1 = img_cropped[:,:,0]\n",
    "        ch2 = img_cropped[:,:,1]\n",
    "        ch3 = img_cropped[:,:,2]\n",
    "\n",
    "        nxblocks = (ch1.shape[1]//pix_per_cell) - 1\n",
    "        nyblocks = (ch1.shape[0]//pix_per_cell) - 1\n",
    "        window = 64\n",
    "        nblock_per_window = (window // pix_per_cell) - 1\n",
    "        cells_per_step = 2\n",
    "        nxsteps = (nxblocks - nblock_per_window) // cells_per_step\n",
    "        nysteps = (nyblocks - nblock_per_window) // cells_per_step\n",
    "\n",
    "        hog1 = get_hog_features(ch1, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "        hog2 = get_hog_features(ch2, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "        hog3 = get_hog_features(ch3, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "\n",
    "        for xb in range(nxsteps):\n",
    "            for yb in range(nysteps):\n",
    "\n",
    "                test_features = []\n",
    "                ypos = yb*cells_per_step\n",
    "                xpos = xb*cells_per_step\n",
    "                xleft = xpos*pix_per_cell\n",
    "                ytop = ypos*pix_per_cell\n",
    "\n",
    "                subimg = cv2.resize(img_cropped[ytop:ytop+window, xleft:xleft+window], (64,64))\n",
    "                spatial_features = bin_spatial(subimg, size=spatial_size)\n",
    "                test_features.append(spatial_features)\n",
    "                hist_features = color_hist(subimg, nbins=hist_bins)\n",
    "                test_features.append(hist_features)\n",
    "\n",
    "                hog_feat1 = hog1[ypos:ypos+nblock_per_window, xpos:xpos+nblock_per_window].ravel()\n",
    "                hog_feat2 = hog2[ypos:ypos+nblock_per_window, xpos:xpos+nblock_per_window].ravel()\n",
    "                hog_feat3 = hog3[ypos:ypos+nblock_per_window, xpos:xpos+nblock_per_window].ravel()\n",
    "                hog_features = np.hstack((hog_feat1, hog_feat2, hog_feat3))\n",
    "                test_features.append(hog_features)\n",
    "\n",
    "                test_features = np.concatenate(test_features)\n",
    "                test_features = X_scaler.transform(test_features)\n",
    "\n",
    "                test_prediction = svc.decision_function(test_features)\n",
    "\n",
    "                if test_prediction > 0.75:\n",
    "                    xbox_left = np.int(xleft*scale)\n",
    "                    ytop_draw = np.int(ytop*scale)\n",
    "                    win_draw = np.int(window*scale)\n",
    "                    heat_map[ytop_draw+y_start_stop[0]:ytop_draw+y_start_stop[0]+win_draw,xbox_left:xbox_left+win_draw] += 1\n",
    "\n",
    "    heat_map[heat_map <= 1] = 0\n",
    "    labels = label(heat_map)\n",
    "    draw_img = draw_labeled_bboxes(draw_img, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a visualization of our heat map and we can clearly see that areas of high intensity correspond to vehicles.\n",
    "<figure>\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/heatmap.png\" width=\"700\"\n",
    "alt=\"Car Image\" />\n",
    "</p>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Implementation\n",
    "\n",
    "We run this pipeline on each image in our video stream to get this final result.\n",
    "\n",
    "[![IMAGE ALT TEXT\n",
    "HERE](https://img.youtube.com/vi/DKyayI2gE2M/0.jpg)](https://www.youtube.com/watch?v=DKyayI2gE2M)\n",
    "\n",
    "One extra thing I did for our video stream is that I used the heat maps of previous images to get stable boxes around the vehicles. I kept a buffer of 10 heat maps which I summed and thresholded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algortihm successfully identifying vehicles when they are in range however there is much room for improvement. For future work one could: \n",
    "* Create a vehicle class and keep track of which pixels are associated with which vehicles. \n",
    "* A pitfall of the current algortihm is that the classifier was not trained on different weather conditions and that may make it act unpredictably. \n",
    "* The algorithm can also be further trained to differentiate between vehicle types. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
